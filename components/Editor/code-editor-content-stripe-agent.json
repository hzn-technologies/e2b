{
  "content": "import asyncio\r\nimport math\r\nimport uuid\r\nimport chevron\r\n\r\n# Monkey patch chevron to not escape html\r\nchevron.render.__globals__[\"_html_escape\"] = lambda string: string\r\n\r\nfrom typing import Any, Dict, List, Literal, Tuple, cast\r\nfrom langchain.agents.tools import InvalidTool\r\nfrom langchain.callbacks.base import AsyncCallbackManager\r\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\r\nfrom langchain.chains.llm import LLMChain\r\nfrom langchain.schema import BaseMessage, SystemMessage, HumanMessage\r\nfrom langchain.tools import BaseTool\r\nfrom langchain.prompts.chat import (\r\n    AIMessagePromptTemplate,\r\n    BaseMessagePromptTemplate,\r\n    ChatPromptTemplate,\r\n)\r\n\r\nfrom .memory import get_memory\r\nfrom agent.base import (\r\n    AgentInteraction,\r\n    OnLogs,\r\n    OnInteractionRequest,\r\n)\r\nfrom agent.output.parse_output import ToolLog\r\nfrom agent.output.output_stream_parser import OutputStreamParser, Step\r\nfrom agent.output.token_callback_handler import TokenCallbackHandler\r\nfrom agent.output.work_queue import WorkQueue\r\nfrom agent.tools.base import create_tools\r\nfrom models.base import ModelConfig, PromptPart, get_model\r\nfrom agent.base import AgentBase, AgentInteractionRequest, GetEnvs\r\nfrom session import NodeJSPlayground\r\n\r\n\r\nclass RewriteStepsException(Exception):\r\n    pass\r\n\r\n\r\nclass BasicAgent(AgentBase):\r\n    max_run_time = 60 * 60 * 24  # in seconds\r\n\r\n    def __init__(\r\n        self,\r\n        config: Any,\r\n        get_envs: GetEnvs,\r\n        on_logs: OnLogs,\r\n        on_interaction_request: OnInteractionRequest,\r\n    ):\r\n        super().__init__()\r\n        self._start_loop: asyncio.Task | None = None\r\n        self.get_envs = get_envs\r\n        self.config = ModelConfig(**config)\r\n        self.on_interaction_request = on_interaction_request\r\n        self.on_logs = on_logs\r\n        self.should_pause = False\r\n        self.canceled = False\r\n        self.rewriting_steps = False\r\n        self.llm_generation = None\r\n        self.can_resume = asyncio.Event()\r\n\r\n    @classmethod\r\n    async def create(\r\n        cls,\r\n        config: Any,\r\n        get_envs: GetEnvs,\r\n        on_logs: OnLogs,\r\n        on_interaction_request: OnInteractionRequest,\r\n    ):\r\n        return cls(\r\n            config,\r\n            get_envs,\r\n            on_logs,\r\n            on_interaction_request,\r\n        )\r\n\r\n    @staticmethod\r\n    async def _run_tool(\r\n        tool_log: ToolLog,\r\n        name_to_tool_map: Dict[str, BaseTool],\r\n    ) -> str:\r\n        # Otherwise we lookup the tool\r\n        if tool_log[\"tool_name\"] in name_to_tool_map:\r\n            tool = name_to_tool_map[tool_log[\"tool_name\"]]\r\n            # We then call the tool on the tool input to get an observation\r\n            observation = await tool.arun(\r\n                tool_log[\"tool_input\"],\r\n                verbose=True,\r\n            )\r\n        else:\r\n            observation = await InvalidTool().arun(  # type: ignore\r\n                tool_name=tool_log[\"tool_name\"],\r\n                tool_input=tool_log[\"tool_input\"],\r\n                verbose=True,\r\n            )\r\n        return observation\r\n\r\n    @staticmethod\r\n    def _get_prompt_part(\r\n        prompts: List[PromptPart],\r\n        role: Literal[\"user\", \"system\"],\r\n        type: str,\r\n    ):\r\n        return (\r\n            next(\r\n                prompt\r\n                for prompt in prompts\r\n                if prompt.role == role and prompt.type == type\r\n            )\r\n            # Double the parenthesses to escape them because the string will be prompt templated.\r\n            .content\r\n        )\r\n\r\n    @staticmethod\r\n    def _create_prompt(\r\n        config: ModelConfig,\r\n        tools: List[BaseTool],\r\n        instructions: Any,\r\n    ):\r\n        system_template = chevron.render(\r\n            \"\\n\\n\".join(\r\n                [\r\n                    BasicAgent._get_prompt_part(config.prompt, \"system\", \"prefix\"),\r\n                    \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\r\n                    BasicAgent._get_prompt_part(config.prompt, \"system\", \"suffix\"),\r\n                ]\r\n            ),\r\n            instructions,\r\n        )\r\n\r\n        human_template = chevron.render(\r\n            \"\\n\\n\".join(\r\n                [\r\n                    BasicAgent._get_prompt_part(config.prompt, \"user\", \"prefix\"),\r\n                ]\r\n            ),\r\n            instructions,\r\n        )\r\n\r\n        messages: List[\r\n            BaseMessage | BaseMessagePromptTemplate | SystemMessage | HumanMessage\r\n        ] = [\r\n            SystemMessage(content=system_template),\r\n            HumanMessage(content=human_template),\r\n            AIMessagePromptTemplate.from_template(\"{repo_context}\"),\r\n            AIMessagePromptTemplate.from_template(\"{agent_scratchpad}\"),\r\n        ]\r\n        return ChatPromptTemplate(\r\n            input_variables=[\"agent_scratchpad\", \"repo_context\"],\r\n            messages=messages,\r\n        )\r\n\r\n    async def _run(self, instructions: Any):\r\n        playground = None\r\n        try:\r\n            # -----\r\n            # Initialize callback manager and handler for controlling token stream\r\n            callback_manager = AsyncCallbackManager([StreamingStdOutCallbackHandler()])\r\n            self.token_handler = TokenCallbackHandler()\r\n            callback_manager.add_handler(self.token_handler)\r\n\r\n            # -----\r\n            # Create tools\r\n            # Create playground for code tools\r\n            playground = NodeJSPlayground(get_envs=self.get_envs)\r\n\r\n            await playground.checkout_repo(instructions[\"RepoURL\"], \"\/repo\")\r\n\r\n            tools = list(create_tools(playground=playground))\r\n            self.tool_names = [tool.name for tool in tools]\r\n            tool_map = {tool.name: tool for tool in tools}\r\n\r\n            # Assign callback manager to tools\r\n            for tool in tools:\r\n                tool.callback_manager = callback_manager\r\n\r\n            # -----\r\n            # Create LLM from specified models\r\n            prompt = self._create_prompt(self.config, tools, instructions)\r\n            llm_chain = LLMChain(\r\n                llm=get_model(self.config, callback_manager),\r\n                prompt=prompt,\r\n                callback_manager=callback_manager,\r\n                verbose=True,\r\n            )\r\n\r\n            repo_files: List[Tuple[str, str]] = []\r\n            async for file in playground.get_files(\r\n                \"\/repo\",\r\n                [\"node_modules\", \".git\", \"package-lock.json\", \".next\"],\r\n            ):\r\n                repo_files.append(file)\r\n\r\n            vectordb = await get_memory(self.config, repo_files)\r\n\r\n            async def get_repo_context(prompt: str):\r\n                docs = await vectordb.asimilarity_search(\r\n                    prompt,\r\n                    k=min(5, len(repo_files)),\r\n                )\r\n\r\n                print(\"getting docs >>>\", [doc.metadata[\"path\"] for doc in docs])\r\n\r\n                return \"Relevant files in the initial codebase:\\n\\n\" + \"\\n\".join(\r\n                    [\r\n                        f\"{doc.metadata['path']}\\n```\\n({doc.page_content})\\n```\\n\"\r\n                        for doc in docs\r\n                    ]\r\n                )\r\n\r\n            # -----\r\n            # Create log handlers\r\n            # Used for sending logs to db\/frontend without blocking\r\n            steps_streamer = WorkQueue[List[Step]](\r\n                on_workload=lambda steps: self.on_logs(steps),\r\n            )\r\n\r\n            def stream_steps(steps: List[Step]):\r\n                steps_streamer.schedule(steps)\r\n\r\n            # Used for parsing token stream into logs+actions\r\n            self._output_parser = OutputStreamParser(tool_names=self.tool_names)\r\n\r\n            # This function is used to inject information from previous steps to the current prompt\r\n            # TODO: Improve the scratchpad handling and prompts for templates\r\n            def get_agent_scratchpad():\r\n                steps = self._output_parser.get_steps()\r\n\r\n                if (\r\n                    len(steps) == 0\r\n                    or len(steps) == 1\r\n                    and not self._output_parser._token_buffer\r\n                ):\r\n                    return \"\"\r\n\r\n                agent_scratchpad = \"\"\r\n                for step in steps:\r\n                    agent_scratchpad += step[\"output\"]\r\n\r\n                    tool_logs = (\r\n                        cast(ToolLog, action)\r\n                        for action in step[\"logs\"]\r\n                        if action[\"type\"] == \"tool\"\r\n                    )\r\n\r\n                    tool_outputs = \"\\n\".join(\r\n                        log.get(\"tool_output\", \"\")\r\n                        for log in tool_logs\r\n                        if log.get(\"tool_output\")\r\n                    )\r\n\r\n                    if tool_outputs:\r\n                        agent_scratchpad += f\"\\nObservation: {tool_outputs}\\nThought:\"\r\n\r\n                return f\"This is my previous work (I will continue where I left off):\\n{agent_scratchpad}\"\r\n\r\n            print(\"Generating...\", flush=True)\r\n            while True:\r\n                try:\r\n                    await self._check_interrupt()\r\n                    # Query the LLM in background\r\n                    scratchpad = get_agent_scratchpad()\r\n                    repo_context = await get_repo_context(\r\n                        prompt=prompt.format_prompt(\r\n                            agent_scratchpad=scratchpad, repo_context=\"\"\r\n                        ).to_string(),\r\n                    )\r\n\r\n                    self.llm_generation = asyncio.create_task(\r\n                        llm_chain.agenerate(\r\n                            [\r\n                                {\r\n                                    \"repo_context\": repo_context,\r\n                                    \"agent_scratchpad\": scratchpad,\r\n                                    \"stop\": \"Observation:\",\r\n                                }\r\n                            ]\r\n                        )\r\n                    )\r\n                    while True:\r\n                        await self._check_interrupt()\r\n                        # Consume the tokens from LLM\r\n                        token = await self.token_handler.retrieve_token()\r\n\r\n                        if token is None:\r\n                            break\r\n\r\n                        await self._check_interrupt()\r\n                        self._output_parser.ingest_token(token)\r\n                        stream_steps(self._output_parser.get_steps())\r\n\r\n                    current_step = self._output_parser.get_current_step()\r\n\r\n                    # Check if the run is finished\r\n                    if any(\r\n                        keyword in current_step[\"output\"]\r\n                        for keyword in (\"Final Answer\", \"FinalAnswer\", \"Finished\")\r\n                    ):\r\n                        break\r\n\r\n                    for tool in (\r\n                        cast(ToolLog, action)\r\n                        for action in current_step[\"logs\"]\r\n                        if action[\"type\"] == \"tool\"\r\n                    ):\r\n                        await self._check_interrupt()\r\n                        tool_output = await self._run_tool(\r\n                            tool,\r\n                            name_to_tool_map=tool_map,\r\n                        )\r\n                        await self._check_interrupt()\r\n                        self._output_parser.ingest_tool_output(tool_output)\r\n                        stream_steps(self._output_parser.get_steps())\r\n\r\n                except RewriteStepsException:\r\n                    print(\"REWRITING\")\r\n                    continue\r\n            # await playground.push_repo()\r\n        except:\r\n            raise\r\n        finally:\r\n            if playground is not None:\r\n                playground.close()\r\n            await self.on_interaction_request(\r\n                AgentInteractionRequest(\r\n                    interaction_id=str(uuid.uuid4()),\r\n                    type=\"done\",\r\n                )\r\n            )\r\n\r\n    async def _check_interrupt(self):\r\n        if self.canceled:\r\n            raise Exception(\"Canceled\")\r\n\r\n        if self.should_pause:\r\n            await self.can_resume.wait()\r\n            self.can_resume.clear()\r\n\r\n        if self.rewriting_steps:\r\n            self.rewriting_steps = False\r\n            raise RewriteStepsException()\r\n\r\n    async def _start(self, instructions: Any):\r\n        print(\"Start agent run\")\r\n\r\n        async def start_with_timeout():\r\n            try:\r\n                self._start_loop = asyncio.create_task(\r\n                    self._run(instructions=instructions),\r\n                )\r\n\r\n                await asyncio.wait_for(\r\n                    self._start_loop,\r\n                    timeout=self.max_run_time,\r\n                )\r\n            except asyncio.TimeoutError:\r\n                await self.stop()\r\n                print(\"Timeout\")\r\n\r\n        asyncio.create_task(\r\n            start_with_timeout(),\r\n        )\r\n\r\n    async def _pause(self):\r\n        print(\"Pause agent run\")\r\n        self.should_pause = True\r\n\r\n    async def _resume(self):\r\n        print(\"Resume agent run\")\r\n        self.should_pause = False\r\n        self.can_resume.set()\r\n\r\n    async def _rewrite_steps(self, steps: List[Step]):\r\n        print(\"Rewrite agent run steps\")\r\n        await self._pause()\r\n        self.rewriting_steps = True\r\n        if self.llm_generation:\r\n            self.llm_generation.cancel()\r\n        # TODO: Communicate the rewriting in model prompt - inform about what was rewritten.\r\n        # TODO: Handle token that is generated after a rewrite - if the current step seems \"finished\" the model still outputs string (usually something like . or :).\r\n        self._output_parser = OutputStreamParser(\r\n            tool_names=self.tool_names,\r\n            steps=steps[:-1],\r\n            buffered_step=steps[-1],\r\n        )\r\n        await self._resume()\r\n\r\n    async def interaction(self, interaction: AgentInteraction):\r\n        print(\"Agent interaction\")\r\n        match interaction.type:\r\n            case \"pause\":\r\n                await self._pause()\r\n            case \"resume\":\r\n                await self._resume()\r\n            case \"start\":\r\n                await self._start(interaction.data[\"instructions\"])\r\n            case \"rewrite_steps\":\r\n                await self._rewrite_steps(interaction.data[\"steps\"])\r\n            case _:\r\n                raise Exception(f\"Unknown interaction action: {interaction.type}\")\r\n\r\n    async def stop(self):\r\n        print(\"Cancel agent run\")\r\n        self.canceled = True\r\n        if self.llm_generation:\r\n            self.llm_generation.cancel()\r\n        if self._start_loop:\r\n            self._start_loop.cancel()\r\n"
}